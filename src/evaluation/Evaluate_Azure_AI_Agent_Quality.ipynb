{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea8f7b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet azure-ai-projects==1.0.0b9 azure-identity azure-ai-evaluation dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88ed548e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e993925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects.models import FunctionTool, ToolSet\n",
    "\n",
    "# Import your custom functions to be used as Tools for the Agent\n",
    "from user_functions import user_functions\n",
    "\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"],\n",
    ")\n",
    "\n",
    "AGENT_NAME = \"Seattle Tourist Assistant\"\n",
    "\n",
    "# Add Tools to be used by Agent\n",
    "functions = FunctionTool(user_functions)\n",
    "\n",
    "toolset = ToolSet()\n",
    "toolset.add(functions)\n",
    "\n",
    "# To enable tool calls executed automatically\n",
    "project_client.agents.enable_auto_function_calls(toolset=toolset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce10f184",
   "metadata": {},
   "source": [
    "### Create an AI agent (Azure AI Agent Service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e796bb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created agent, ID: asst_j72kXPo61Vhmnh3gKLURupIW\n"
     ]
    }
   ],
   "source": [
    "agent = project_client.agents.create_agent(\n",
    "    model=os.environ[\"AGENT_MODEL_DEPLOYMENT_NAME\"],\n",
    "    name=AGENT_NAME,\n",
    "    instructions=\"You are a helpful assistant\",\n",
    "    toolset=toolset,\n",
    ")\n",
    "\n",
    "print(f\"Created agent, ID: {agent.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c074f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread for the agent to interact with\n",
    "thread = project_client.agents.create_thread()\n",
    "print(f\"Created thread, ID: {thread.id}\")\n",
    "\n",
    "# Create message to thread\n",
    "\n",
    "MESSAGE = \"Can you email me weather info for Tokyo ?\"\n",
    "\n",
    "message = project_client.agents.create_message(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=MESSAGE,\n",
    ")\n",
    "print(f\"Created message, ID: {message.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "673f8495",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Execute the Agent run\n",
    "run = project_client.agents.create_and_process_run(thread_id=thread.id, agent_id=agent.id)\n",
    "\n",
    "print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "if run.status == \"failed\":\n",
    "    print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "print(f\"Run ID: {run.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "544faf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in project_client.agents.list_messages(thread.id, order=\"asc\").data:\n",
    "    print(f\"Role: {message.role}\")\n",
    "    print(f\"Content: {message.content[0].text.value}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a31e06",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb044c42",
   "metadata": {},
   "source": [
    "### Get data from agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d478a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import AIAgentConverter\n",
    "import json\n",
    "\n",
    "\n",
    "# Initialize the converter that will be backed by the project.\n",
    "converter = AIAgentConverter(project_client)\n",
    "\n",
    "thread_id = thread.id\n",
    "run_id = run.id\n",
    "file_name = \"evaluation_data.jsonl\"\n",
    "\n",
    "# Get a single agent run data\n",
    "evaluation_data_single_run = converter.convert(thread_id=thread_id, run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23ad5a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to save thread data to a JSONL file for evaluation\n",
    "# Save the agent thread data to a JSONL file\n",
    "evaluation_data = converter.prepare_evaluation_data(thread_ids=thread_id, filename=file_name)\n",
    "print(json.dumps(evaluation_data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e5719",
   "metadata": {},
   "source": [
    "### Setting up evaluator\n",
    "\n",
    "We will select the following evaluators to assess the different aspects relevant for agent quality: \n",
    "\n",
    "- [Intent resolution](https://aka.ms/intentresolution-sample): measures the extent of which an agent identifies the correct intent from a user query. Scale: integer 1-5. Higher is better.\n",
    "- [Tool call accuracy](https://aka.ms/toolcallaccuracy-sample): evaluates the agent’s ability to select the appropriate tools, and process correct parameters from previous steps. Scale: float 0-1. Higher is better.\n",
    "- [Task adherence](https://aka.ms/taskadherence-sample): measures the extent of which an agent’s final response adheres to the task based on its system message and a user query. Scale: integer 1-5. Higher is better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0396b475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:azure.ai.evaluation._common._experimental:Class IntentResolutionEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "WARNING:azure.ai.evaluation._common._experimental:Class ToolCallAccuracyEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "WARNING:azure.ai.evaluation._common._experimental:Class TaskAdherenceEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import (\n",
    "    ToolCallAccuracyEvaluator,\n",
    "    AzureOpenAIModelConfiguration,\n",
    "    IntentResolutionEvaluator,\n",
    "    TaskAdherenceEvaluator,\n",
    ")\n",
    "from pprint import pprint\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n",
    ")\n",
    "# Needed to use content safety evaluators\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "    \"project_name\": os.environ[\"PROJECT_NAME\"],\n",
    "    \"resource_group_name\": os.environ[\"RESOURCE_GROUP_NAME\"],\n",
    "}\n",
    "\n",
    "intent_resolution = IntentResolutionEvaluator(model_config=model_config)\n",
    "\n",
    "tool_call_accuracy = ToolCallAccuracyEvaluator(model_config=model_config)\n",
    "\n",
    "task_adherence = TaskAdherenceEvaluator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f981a050",
   "metadata": {},
   "source": [
    "### Run Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a04fd460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-28 08:11:30 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-06-28 08:11:30 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-06-28 08:11:30 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_tool_call_accuracy_20250628_081130_861506, log path: /root/.promptflow/.runs/azure_ai_evaluation_evaluators_tool_call_accuracy_20250628_081130_861506/logs.txt\n",
      "[2025-06-28 08:11:30 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_intent_resolution_20250628_081130_862089, log path: /root/.promptflow/.runs/azure_ai_evaluation_evaluators_intent_resolution_20250628_081130_862089/logs.txt\n",
      "[2025-06-28 08:11:30 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-06-28 08:11:30 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_task_adherence_20250628_081130_862728, log path: /root/.promptflow/.runs/azure_ai_evaluation_evaluators_task_adherence_20250628_081130_862728/logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-28 08:11:35 +0000     717 execution.bulk     INFO     Finished 1 / 6 lines.\n",
      "2025-06-28 08:11:35 +0000     717 execution.bulk     INFO     Average execution time for completed lines: 4.97 seconds. Estimated time for incomplete lines: 24.85 seconds.\n",
      "2025-06-28 08:11:35 +0000     717 execution.bulk     INFO     Finished 2 / 6 lines.\n",
      "2025-06-28 08:11:35 +0000     717 execution.bulk     INFO     Average execution time for completed lines: 2.51 seconds. Estimated time for incomplete lines: 10.04 seconds.\n",
      "2025-06-28 08:11:36 +0000     717 execution.bulk     INFO     Finished 3 / 6 lines.\n",
      "2025-06-28 08:11:36 +0000     717 execution.bulk     INFO     Average execution time for completed lines: 2.01 seconds. Estimated time for incomplete lines: 6.03 seconds.\n",
      "2025-06-28 08:11:37 +0000     717 execution.bulk     INFO     Finished 4 / 6 lines.\n",
      "2025-06-28 08:11:37 +0000     717 execution.bulk     INFO     Average execution time for completed lines: 1.54 seconds. Estimated time for incomplete lines: 3.08 seconds.\n",
      "2025-06-28 08:11:38 +0000     717 execution.bulk     INFO     Finished 5 / 6 lines.\n",
      "2025-06-28 08:11:38 +0000     717 execution.bulk     INFO     Average execution time for completed lines: 1.42 seconds. Estimated time for incomplete lines: 1.42 seconds.\n",
      "2025-06-28 08:11:39 +0000     717 execution.bulk     INFO     Finished 6 / 6 lines.\n",
      "2025-06-28 08:11:39 +0000     717 execution.bulk     INFO     Average execution time for completed lines: 1.46 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "response = evaluate(\n",
    "    data=file_name,\n",
    "    evaluators={\n",
    "        \"tool_call_accuracy\": tool_call_accuracy,\n",
    "        \"intent_resolution\": intent_resolution,\n",
    "        \"task_adherence\": task_adherence,\n",
    "    },\n",
    "    azure_ai_project={\n",
    "        \"subscription_id\": os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "        \"project_name\": os.environ[\"PROJECT_NAME\"],\n",
    "        \"resource_group_name\": os.environ[\"RESOURCE_GROUP_NAME\"],\n",
    "    },\n",
    ")\n",
    "pprint(f'AI Foundary URL: {response.get(\"studio_url\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e2904",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
